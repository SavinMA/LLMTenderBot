services:
  llmtenderbot:
    build: .
    container_name: llmtenderbot
    restart: unless-stopped
    environment:
      - TELEGRAM_BOT_TOKEN=${TELEGRAM_BOT_TOKEN}
      - ANALYZER_TYPE=${ANALYZER_TYPE}
      - LLM_MODEL=${LLM_MODEL}
      - LLM_HOST=${LLM_HOST}
      - MISTRAL_API_KEY=${MISTRAL_API_KEY}
      - MISTRAL_MODEL=${MISTRAL_MODEL}
    env_file:
      - .env
    volumes:
      - ./logs:/app/logs
      - ./data:/app/data  # For persistent data if needed
    networks:
      - llmtenderbot-network
    depends_on:
      - ollama
    healthcheck:
      test: ["CMD", "python", "-c", "import sys; sys.exit(0)"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_HOST=0.0.0.0
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - llmtenderbot-network
    # Uncomment the following lines if you have a GPU
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Optional: Redis for caching (if needed in future)
  # redis:
  #   image: redis:7-alpine
  #   container_name: redis
  #   restart: unless-stopped
  #   ports:
  #     - "6379:6379"
  #   volumes:
  #     - redis_data:/data
  #   networks:
  #     - llmtenderbot-network

volumes:
  ollama_data:
    driver: local
  # redis_data:
  #   driver: local

networks:
  llmtenderbot-network:
    driver: bridge 